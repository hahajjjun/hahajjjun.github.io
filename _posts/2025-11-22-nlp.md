---
layout: post
title:  "Nature's Language Processing"
subtitle: ""
excerpt_separator: "<!--more-->"
date:	2025-11-22
comments: true
categories: ["Annotated BI"]
---

### Testbed for benchmarking large models trained on biological sequences
My recent [blog post](https://hahajjjun.github.io/annotated%20bi/2025/09/06/fm-sb.html) discusses the tough criteria that challenge large models trained on biological sequences.
For simplicity, I'd like to refer to "large models trained on biological sequences" as *NLP* models, which abbreviates "Nature's Language Processing".

In particular, we aspire for *NLP* models to achieve:
1) robust generalization beyond pretraining objectives
2) reproduction of established biological principles without task-specific tuning

There still exists a semantic gap between biologists and computer scientists in benchmarking *NLP* models across multiple tasks.
Therfore, we will now discuss major advancements in evaluating *NLP* models through the careful design of biologically principled tasks.

---
### Central Dogma
Let's start from the central dogma, a core principle across three world of DNA, RNA, and protein.

