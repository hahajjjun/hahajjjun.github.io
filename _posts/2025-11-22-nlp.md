---
layout: post
title:  "Nature's Language Processing"
subtitle: ""
excerpt_separator: "<!--more-->"
date:	2025-11-22
comments: true
categories: ["Annotated BI"]
---

### Testbed for benchmarking large models trained on biological sequences
My recent [blog post](https://hahajjjun.github.io/annotated%20bi/2025/09/06/fm-sb.html) discusses the tough criteria that challenge large models trained on biological sequences.

For simplicity, I'd like to refer to "large models trained on biological sequences" as *NLP* models, which abbreviates "Nature's Language Processing".

In particular, we aspire for *NLP* models to achieve:
1) robust generalization beyond pretraining objectives
2) reproduction of established biological principles without task-specific tuning

There still exists a semantic gap between biologists and computer scientists in benchmarking *NLP* models across multiple tasks.

Therfore, we will now discuss major advancements in evaluating *NLP* models through the careful design of biologically principled tasks.

---
### Central Dogma
Let's start from the central dogma, a core principle across three world of DNA, RNA, and protein.

[Wang et al.](https://www.biorxiv.org/content/10.1101/2025.06.25.661622v1.full) provides a benchmark designed with extensive tasks accounting for the central dogma.

Here, few examples of sequence(DNA, RNA, Protein) level tasks are presented below among the full list of 36 tasks proposed at the original work.

|Modality|Tasks|Description|Category|
|:-:|:-:|---|:-:|
|DNA|Promoter annotation|Distinguishing promoter regions <br/>1) TATA box, Initiator, CCAAT box, GC box <br/>2) Minimal seqeuence for transcription initiation around TSS|Classification|
|DNA|Enhancer types classification|Distinguishing <br/>1) Tissue-specific enhancers <br/>2) Tissue-invariant enhancers <br/>3) Non-enhancer regions|Classification|
|RNA|mRNA stability prediction|Predicting mRNA stability profile|Regression|
|RNA|SARS-CoV-2 Vaccine degradation prediction|Predict hydrolysis rates of mRNA|Regression|
|Protein|Post-translational modification prediction|Predict PTM categories|Classification|
|Protein|Fluorescence prediction|Predict log-flourescence for avGFP sequences|Regression|

We can notice that the scope of the task largely varies. 

Some tasks are testing for general principles and regulatory syntaxes of gene expression while others tests for particular biological property(e.g. degradation of mRNA, fluorescence of protein). 

This naturally pose a constraint about zero-shot predictability: we can expect that for some tasks, *NLP* models will easily succeed with minimal fine-tuning. 

On the other hand, some tasks are intrinsically designed with narrow-scopes thus requires adjustment ranging from linear probing to extensive fine-tuning of all layers.
