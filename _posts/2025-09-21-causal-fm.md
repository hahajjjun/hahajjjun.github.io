---
layout: post
title:  "Coalescing randomized experiments and foundation models"
subtitle: ""
excerpt_separator: "<!--more-->"
date:	2025-09-21
comments: true
categories: ["Annotated BI"]
---

### Randomized experiments and average treatment effect(ATE)
Consider a dataset $\mathcal{D}$, containing $n$ tuples $(X,Y,A)$: $X$ be the covariates, $Y$ be the outcome, and $A \in \{0,1\}$ be the treatment variable. We assume that

1. The data is sampled i.i.d from joint distribution $\mathbb{P}$ over $(X,Y,A)$
2. $Y(a) \perp A$, for $a=0,1$
3. $\mathbb{P}(A=a)=\pi_a>0$, for $a=0,1$, and propesnity score $\pi_a$ is known.
4. SUTVA(stable unit treatment value assumption) holds: $Y_i=Y_i(A_i)$

Under these assumptions, we aim to estimate average treatment effect(ATE):
$$\theta := \mathbb{E}[Y(1)-Y(0)] = \mathbb{E}[Y\mid A=1]-\mathbb{E}[Y\mid A=0]$$

Standard approach is to estimate $\theta$ using difference in means (DM) estimator:
$\hat{\theta}\_{DM} = \frac{1}{n_1} \sum_{i;A_i=1} {Y_i} - \frac{1}{n_0} \sum_{i;A_i=0} {Y_i}$, where
$n_i$ denotes the number of data samples with $A_i=a$.

### Estimators of ATE

**Difference in means estimator $\hat{\theta}_{DM}$**

For difference in means(DM) estimator $\theta_{DM}$, it is known that the estimator is consistent and asymptotically normal, i.e. $\sqrt{n}(\hat{\theta}\_{DM}-\theta) \rightarrow^d \mathcal{N}(0, V_{DM})$ with asymptotic variance $V_{DM}$. 
We also have a consistent estimator of the asymptotic variance, $\hat{V}\_{DM}=V_{DM}+o(1)$(see [Wager](https://web.stanford.edu/~swager/causal_inf_book.pdf), Theorem 1.2). 
We therefore can construct asymptotically valid CI:

$$\mathcal{C}^\alpha_{DM} = (\hat{\theta}\_{DM} \pm z_{1-\frac{\alpha}{2}}\sqrt{\frac{\hat{V}_{DM}}{n}})$$

**Augmented Inverse Probability Weighting estimator $\hat{\theta}_{AIPW}$**

Let's start with the unbiasedness of the AIPW estimator. 

AIPW estimator is derived from the IPW estimator, which  weights each sample from untreated and treated groups by the inverse of estimated propensity score.

$$\hat{\theta}_{IPW}=\frac{1}{n}\sum_i\{\frac{A_iY_i}{\hat{\pi(X_i)}}-\frac{(1-A_i)Y_i}{1-\hat{\pi(X_i)}}\}$$

If we know the propensity score, we replace $\hat{\pi}(X_i) \leftarrow \pi_1$ and $1-\hat{\pi}(X_i) \leftarrow \pi_0$ and yields an unbiased estimator of $\theta$. 
We can show the unbiasedness from $\mathbb{E}[A_iY_i]=\mathbb{E}[Y_i\mid A_i=1]\cdot\pi_1$ and $\mathbb{E}[(1-A_i)Y_i]=\mathbb{E}[Y_i\mid A_i=0]\cdot\pi_0$.

+) Note that if we estimate the propensity score consistently, IPW estimator is consistent for the $\theta$.

The AIPW estimator can be expressed as a sum of IPW estimator and the adjustment term.

$$\hat{\theta}\_{AIPW}=\hat{\theta}\_{IPW}-\frac{1}{n} \sum_i \frac{(A_i-\hat{\pi}(X_i))}{\hat{\pi}(X_i)(1-\hat{\pi}(X_i))} \cdot [(1-\hat{\pi}(X_i))\cdot h(X_i,1) + \hat{\pi}(X_i)\cdot h(X_i,0)]$$

For the case which propensity score is known, $\hat{\theta}_{AIPW}=\frac{1}{n} \sum_i \psi_i(h)$, where $\psi_i(h) := (\frac{A_i}{\pi_1}(Y_i-h(X_i,1))+h(X_i,1)) - (\frac{1-A_i}{\pi_0} (Y_i-h(X_i,0))+h(X_i,0))$.

Here, $h(\cdot,\cdot)$ is a square-integrable function that performs outcome regression by given covariate $X_i$ and binary treatment indicator $A_i$. 

AIPW estsimator is unbiased when propensity score and outcome model $h(\cdot, \cdot)$ is known since $\mathbb{E}[\frac{A_ih(X_i,1)}{\pi_1}]=\mathbb{E}[h(X_i,1)]$ and $\mathbb{E}[\frac{(1-A_i)h(X_i,0)}{\pi_0}]=\mathbb{E}[h(X_i,0)]$, therefore $\mathbb{E}[\psi_i(h)]=\mathbb{E}[Y_i\mid A_i=1]-\mathbb{E}[Y_i\mid A_i=0]$.

Among the classes of AIPW estimator depending on the outcome model $h$, the most efficient estimator should minimize the asymptotic variance. Specifically, lower bound is attained when $h^*(X,A)=\mathbb{E}[Y\mid X,A]$ : the conditional mean of the outcome. We combine this result with the fact that all estimators of $\theta$ that are consistent and asymptotically normal are asymptoticdally eqivalent to the APIW estimator (when propensity score is known, see [Robins](https://www.tandfonline.com/doi/abs/10.1080/01621459.1994.10476818)). We therefore conclude that $\hat{\theta}_{AIPW}(h^*)$ has the smallest possible CI among all consistent and asymptotically normal estimators of $\theta$(see [Bartolomeis et al.](https://arxiv.org/pdf/2502.04262)).

**Aymptotic behavior of AIPW: we only have $\hat{h}$ in practice**
One practical issue is that we only have estimator $\hat{h}$ of $h^*$, so the efficiency lower bound is achieved only if 

$$\left\lVert \hat{h}-h^{*} \right\rVert_{L_2}=o(1)$$.

This implies that the asymptotic normality is achieved as long as the $\hat{h}$ has an asymptotic limit $h^\dagger$, i.e.

> Proposition 1.1. and proof
