---
layout: post
title:  "Coalescing randomized experiments and foundation models"
subtitle: ""
excerpt_separator: "<!--more-->"
date:	2025-09-21
comments: true
categories: ["Annotated BI"]
---

### Randomized experiments and average treatment effect(ATE)
Consider a dataset $\mathcal{D}$, containing $n$ tuples $(X,Y,A)$: $X$ be the covariates, $Y$ be the outcome, and $A \in \{0,1\}$ be the treatment variable. We assume that

1. The data is sampled i.i.d from joint distribution $\mathbb{P}$ over $(X,Y,A)$
2. $Y(a) \perp A$, for $a=0,1$
3. $\mathbb{P}(A=a)=\pi_a>0$, for $a=0,1$, and propesnity score $\pi_a$ is known.
4. SUTVA(stable unit treatment value assumption) holds: $Y_i=Y_i(A_i)$

Under these assumptions, we aim to estimate average treatment effect(ATE):
$$\theta := \mathbb{E}[Y(1)-Y(0)] = \mathbb{E}[Y|A=1]-\mathbb{E}[Y|A=0]$$

Standard approach is to estimate $\theta$ using difference in means (DM) estimator:
$\hat{\theta}_{DM} = \frac{1}{n_1} \sum_{i;A_i=1} {Y_i} - \frac{1}{n_0} \sum_{i;A_i=0} {Y_i}$, where
$n_i$ denotes the number of data samples with $A_i=a$.

### Estimators of ATE

**Difference in means estimator $\hat{\theta}_{DM}$**

For difference in means(DM) estimator $\theta_{DM}$, it is known that the estimator is consistent and asymptotically normal, i.e. $\sqrt{n}(\hat{\theta}_{DM}-\theta) \rightarrow^d \mathcal{N}(0, V_{DM})$ with asymptotic variance $V_{DM}$. We can also obtain a consistent estimator of the asymptotic variance, $\hat{V}_{DM}=V_{DM}+o(1)$. We therefore can construct asymptotically valid CI:
$$
\mathcal{C}^\alpha_{DM} = (\hat{\theta}_{DM} \pm z_{1-\frac{\alpha}{2}}\sqrt{\frac{\hat{V}_{DM}}{n}})
$$

**Augmented Inverse Probability Weighting estimator $\hat{\theta}_{AIPW}$**

Let's start with the unbiasedness of the AIPW estimator. 

AIPW estimator is derived from the IPW estimator, which  weights each sample from untreated and treated groups by the inverse of estimated propensity score: $\hat{\theta}_{IPW}=\frac{1}{n}\sum_i\{\frac{A_iY_i}{\hat{\pi(X_i)}}-\frac{(1-A_i)Y_i}{1-\hat{\pi(X_i)}}\}$. If we know the propensity score, we replace $\hat{\pi}(X_i) \leftarrow \pi_1$ and $1-\hat{\pi}(X_i) \leftarrow \pi_0$ and this yields an unbiased estimator of $\theta$. This can be shown with the fact that $\mathbb{E}[A_iY_i]=\mathbb{E}[Y_i|A_i=1]\cdot\pi_1$ and $\mathbb{E}[(1-A_i)Y_i]=\mathbb{E}[Y_i|A_i=0]\cdot\pi_0$.

+) Note that if we estimate the propensity score consistently, IPW estimator is consistent for the $\theta$.

Exact formula of the APIW estimator is as follows:
$$
\hat{\theta}_{AIPW}=\hat{\theta}_{IPW}-\frac{1}{n} \sum_i \frac{(A_i-\hat{\pi}(X_i))}{\hat{\pi}(X_i)(1-\hat{\pi}(X_i))} \cdot [(1-\hat{\pi}(X_i))\cdot h(X_i,1) + \hat{\pi}(X_i)\cdot h(X_i,0)]
$$
For the case which propsensity score is known, $\hat{\theta}_{AIPW}=\frac{1}{n} \sum_i \psi_i(h)$, where $\psi_i(h) := (\frac{A_i}{\pi_1}(Y_i-h(X_i,1))+h(X_i,1)) - (\frac{1-A_i}{\pi_0} (Y_i-h(X_i,0))+h(X_i,0))$.

Here, $h(\cdot,\cdot)$ is a square-integrable function that performs outcome regression by given covariate $X_i$ and binary treatment indicator $A_i$. 

We will prove that AIPW estimator is unbiased when propensity score is known, but the most efficient estimator should minimize the asymptotic variance. Specifically, lower bound is attained when $h(X,A)=\mathbb{E}[Y|X,A]$ : the conditional mean of the outcome. In this sense, $h(X_i, A_i)$ can be alternatively expressed as $\hat{\mathbb{E}}[Y_i|X_i,A_i]$.

TODO: Let's start with the proof of unbiaseness of AIPW estiamtor. Question: do we need h() is also known? is arbitrary h okay to make AIPW unbiased?

### Reference

https://web.stanford.edu/~swager/causal_inf_book.pdf

https://scholar.harvard.edu/files/aglynn/files/AIPW.pdf